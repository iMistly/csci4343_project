# -*- coding: utf-8 -*-
"""CSCI4343 - Data Mining Coffee Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cI2z6xKWdzYAwpn7VSZZJ-b29P2WI23w
"""

"""# Task 1:

## 1.1) Visualize the data!
"""

import numpy as np
import matplotlib.pyplot as plt

data = np.load('coffee_combined_data.npz')
coffee_dataset = data['dataset']
coffee_labels = data['labels']

print(coffee_dataset)
print(coffee_labels)

class Experiment:
  def __init__(self, label, data):
    self.label = label
    self.data = data
    self.classification = None

    if self.label == "Sprout":
      self.classification = 0
    elif self.label == "Rwanda":
      self.classification = 1
    else:
      self.classification = -1

  def weight(self):
    return self.data[-1]

  def time(self):
    return len(self.data) * 0.2 # 200ms between each point

  def average_pouring_rate(self):
    return self.weight() / self.time()

  def max_pouring_rate(self):
    # Get max of derivative
    derivative = np.diff(self.data) / 0.2
    return np.max(derivative)

  def time_till_half_weight(self):
    half = self.weight() / 2
    for i in range(len(self.data)):
      if self.data[i] >= half:
        return i * 0.2
    return -1

  def get_features(self):
    return [self.weight(), self.time(), self.average_pouring_rate(), self.max_pouring_rate(), self.time_till_half_weight()]

  def print_features(self):
    print(f"Weight: {self.weight()}")
    print(f"Time: {self.time()}")
    print(f"Average Pouring Rate: {self.average_pouring_rate()}")
    print(f"Max Pouring Rate: {self.max_pouring_rate()}")
    print(f"Time Till Half Weight: {self.time_till_half_weight()}")

valid_data = []

for i in range(len(coffee_dataset)):
  valid = True
  last_index = 0
  for j in range(len(coffee_dataset[i])):
    # End of data
    if coffee_dataset[i][j] == -1:
      last_index = j-1
      break
    # Something went wrong with the data
    elif coffee_dataset[i][j] < -1:
      valid = False
      break
  if valid:
    valid_data.append(Experiment(coffee_labels[i], coffee_dataset[i][:last_index]))
  else:
    print(f"Skipping {coffee_labels[i]} {i} due to invalid data")

for experiment in valid_data:
  color = "r" if experiment.label == "Sprout" else "b"
  plt.plot(experiment.data, color)
  plt.plot(len(experiment.data)-1,experiment.data[-1], f"{color}o")
plt.title("Coffee Data")
plt.xlabel("Time (s)")
plt.ylabel("Coffee (mL)")
plt.show()

"""## 1.2) Analyze the Features

The data was a little messy, so we implemented a basic method to rid of the most obvious unusable data. There doesn't appear to be any obvious features that stand out in order to seperate the coffee types. Features we can gather:
* Final weight
* Time taken to pour
* Fastest point of weight gain (max(derivative))

## 1.3) Convert labels to numbers

This has been completed in the creation of the `Experiment` class

# Task 2:

## 2.1) Train MLP Model

Make seperate Tensors
"""

from math import exp
import torch

# Divide the data into their classifications and inserting their net weight as the data
c0, c1 = [], []
for experiment in valid_data:
  # experiment.print_features()
  # print()
  if experiment.classification == 0:
    c0.append(experiment.get_features())
  else:
    c1.append((experiment.get_features()))

c0_tensor = torch.tensor(c0, dtype=torch.float32)
c1_tensor = torch.tensor(c1, dtype=torch.float32)
# print(c0_tensor)
# print(c1_tensor)

"""Create MLP Model child class from torch.nn.Module"""

class MultilayerPerceptron(torch.nn.Module):
  def __init__(self):
    super(MultilayerPerceptron, self).__init__()
    self.dimensions = 3
    self.linear1 = torch.nn.Linear(5, self.dimensions)
    self.linear2 = torch.nn.Linear(self.dimensions, 1)
    self.sigmoid = torch.nn.Sigmoid()

  def forward(self, x):
    x = self.linear1(x)
    x = self.sigmoid(x)
    x = self.linear2(x)
    x = self.sigmoid(x)
    return x

"""Create loss function"""

def negative_likelihood(model, c0, c1):
  o = model(c0)
  likelihood = - torch.log(o+0.000001)
  o = model(c1)
  likelihood2 = - torch.log(1-o+0.000001)
  return likelihood.sum() + likelihood2.sum()

"""Descend the gradient"""

import torch.optim as optim
model = MultilayerPerceptron()
op = optim.SGD(model.parameters(), lr=0.1)
n_epoch = 5000
for epoch in range(n_epoch):
  loss = negative_likelihood(model, c0_tensor, c1_tensor)
  op.zero_grad()
  loss.backward()
  op.step()
  if epoch % (n_epoch/10) == 0:
    print(f"Epoch: {epoch} Loss: {loss.item()}")

"""## 2.2) MLP with allocated training data and test data

Get random holdout samples
"""

size = int(c0_tensor.shape[0] * 0.4)
indices_holdout_c0 = torch.randperm(c0_tensor.shape[0])[:size]
size = int(c1_tensor.shape[0] * 0.4)
indices_holdout_c1 = torch.randperm(c1_tensor.shape[0])[:size]

c0_tensor_holdout = c0_tensor[indices_holdout_c0]
c1_tensor_holdout = c1_tensor[indices_holdout_c1]

"""Remove holdout samples from training data"""

vector_c0 = torch.zeros(c0_tensor.shape[0])
for i in indices_holdout_c0:
  vector_c0[i] = 1
c0_tensor_train = c0_tensor[vector_c0 != 1]

vector_c1 = torch.zeros(c1_tensor.shape[0])
for i in indices_holdout_c1:
  vector_c1[i] = 1
c1_tensor_train = c1_tensor[vector_c1 != 1]

print(c0_tensor_train.shape)
print(c1_tensor_train.shape)
print(c0_tensor_holdout.shape)
print(c1_tensor_holdout.shape)

"""Descend the gradient"""

import torch.optim as optim
model = MultilayerPerceptron()
op = optim.SGD(model.parameters(), lr=0.1)
loss_lst = []
loss_holdout_lst = []
n_epoch = 5000
for epoch in range(n_epoch):
  loss = negative_likelihood(model, c0_tensor_train, c1_tensor_train)
  op.zero_grad()
  loss.backward()
  op.step()

  with torch.no_grad():
    loss_holdout = negative_likelihood(model, c0_tensor_holdout, c1_tensor_holdout)
  loss_lst.append(loss.item())
  loss_holdout_lst.append(loss_holdout.item())
  if epoch % (n_epoch/10) == 0:
    print(f"Epoch: {epoch} Loss: {loss.item()} Loss Holdout: {loss_holdout.item()}")

"""# 2.5) Test unlabeled data

Load test data
"""

test_data = np.load('test_data.npz')
test_experiments = []

"""Convert data into their class varient"""

for x in test_data:
  count = 0
  for y in test_data[x]:
    if y == -1:
      count -= 1
      break
    count += 1
  test_experiments.append(Experiment(x, test_data[x][:count]))

"""Graph data"""

for experiment in test_experiments:
  color = "g"
  plt.plot(experiment.data, color)
  plt.plot(len(experiment.data)-1,experiment.data[-1], f"{color}o")
plt.title("Coffee Data")
plt.xlabel("Time (s)")
plt.ylabel("Coffee (mL)")
plt.show()

"""Throw data into the model"""

for experiment in test_experiments:
  with torch.no_grad():
    prediction = model(torch.tensor(experiment.get_features(), dtype=torch.float32))
    print(f"{experiment.label}: {prediction.item()}")